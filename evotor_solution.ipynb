{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Эвотор Data Challenge. Задача классификации товаров по категориям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Автор: Мартынов Роман"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Данное довольно простое решение дало наилучший результат, которого я смог добиться. Он соответствует 20 месту в финальном рейтинге и равен 0,947752 (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Парсинг исходных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в файлах .csv некоторые строки были разделены на несколько строк, было принято решение вручную распарсить эти файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categories_parse(path: 'str') -> (list, list):\n",
    "    \"\"\"Парсер файла categories.csv\"\"\"\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    file.readline()\n",
    "    s = ''\n",
    "    group_id = []\n",
    "    name = []\n",
    "\n",
    "    for line in file:\n",
    "        # если одна строка csv разнесена на две, то конкатенируем с предыдущей считавшейся\n",
    "        s += line\n",
    "        p = re.compile(r'([\\d]+)'                    # число\n",
    "                       r'[,]'                        # запятая\n",
    "                       r'[\"]'                        # открывающаяся кавычка\n",
    "                       r'([^\"]+)'                    # любые символы, кроме \"\n",
    "                       r'[\"]'                        # закрывающаяся кавычка\n",
    "                       r'[\\w | \\W]+')                # любые символы\n",
    "        m = p.search(s)\n",
    "        if m is None:\n",
    "            pass\n",
    "        else:\n",
    "            s = ''\n",
    "            group_id.append(m.group(1))\n",
    "            name.append(m.group(2))\n",
    "    return group_id, name\n",
    "\n",
    "def train_parse(path: '') -> (list, list, list):\n",
    "    \"\"\"Парсер файла evo_train.csv\"\"\"\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    file.readline()\n",
    "    s = ''\n",
    "    name = []\n",
    "    group_id = []\n",
    "    id = []\n",
    "    \n",
    "    for line in file:\n",
    "        s += line\n",
    "        p = re.compile(r'([\\w | \\W]+)'    # любой символ любое число раз\n",
    "                       r'[,]'             # запятая\n",
    "                       r'([\\d]+)'         # число\n",
    "                       r'[,]'             # запятая\n",
    "                       r'([\\d]+)'         # число\n",
    "                       r'$')              # конец строки\n",
    "        m = p.search(s)\n",
    "        if m is None:\n",
    "            pass\n",
    "        else:\n",
    "            s = ''\n",
    "            name.append(m.group(1))\n",
    "            group_id.append(m.group(2))\n",
    "            id.append(m.group(3))\n",
    "    return name, group_id, id\n",
    "\n",
    "def test_parse(path: '') -> (list, list, list):\n",
    "    \"\"\"Парсер файла evo_test.csv\"\"\"\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    file.readline()\n",
    "    s = ''\n",
    "    name = []\n",
    "    id = []\n",
    "\n",
    "    for line in file:\n",
    "        s += line\n",
    "        p = re.compile(r'([\\w | \\W]+)'    # любой символ любое число раз\n",
    "                       r'[,]'             # запятая\n",
    "                       r'([\\d]+)'         # число\n",
    "                       r'$')              # конец строки\n",
    "        m = p.search(s)\n",
    "        if m is None:\n",
    "            pass\n",
    "        else:\n",
    "            s = ''\n",
    "            name.append(m.group(1))\n",
    "            id.append(m.group(2))\n",
    "    return name, id\n",
    "\n",
    "# парсинг тренировочных данных\n",
    "Data_train, group_id_tr, id_tr = train_parse(r'data\\evo_train.csv')\n",
    "# парсинг тестовых данных\n",
    "Data_test, id_test = test_parse(r'data\\evo_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Предобработка исходных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случаях применения стемминга или лемматизации, качество предсказаний резко падало, поэтому от этих способов пришлось отказаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Text_Preprocessor():\n",
    "\n",
    "    def __init__(self, text):\n",
    "        # текст = [[word, ... , word], [word, ... ,word]]\n",
    "        self.format_text = [line.lower().split(' ') for line in text]\n",
    "\n",
    "    def del_word_with_n_letters(self, n=1):\n",
    "        \"\"\"Удаление слов, состоящих из одной буквы\"\"\"\n",
    "        res_text = []\n",
    "        for line in self.format_text:\n",
    "            line_list = []\n",
    "            for word in line:\n",
    "                if len(word) > n:\n",
    "                    line_list.append(word)\n",
    "            res_text.append(line_list)\n",
    "        self.format_text = res_text\n",
    "\n",
    "    def only_words(self):\n",
    "        \"\"\"Оставляет только слова кириллицей или латиницей\"\"\"\n",
    "        res_text = []\n",
    "        p = re.compile(r'[а-яА-ЯёЁa-zA-Z]+')\n",
    "        for line in self.format_text:\n",
    "            line_list = []\n",
    "            for word in line:\n",
    "                if p.findall(word):\n",
    "                    for elem in p.findall(word):\n",
    "                        line_list.append(elem)\n",
    "            res_text.append(line_list)\n",
    "        self.format_text = res_text\n",
    "\n",
    "    def del_stop_words(self):\n",
    "        \"\"\"Удаляет предлоги\"\"\"\n",
    "        morph = pymorphy2.MorphAnalyzer()\n",
    "        res_text = []\n",
    "        for line in self.format_text:\n",
    "            new_line = []\n",
    "            for word in line:\n",
    "                p = morph.parse(word)[0]\n",
    "                pos = p.tag.POS\n",
    "                if pos not in ['PREP']:\n",
    "                    new_line.append(word)\n",
    "            res_text.append(new_line)\n",
    "        self.format_text = res_text\n",
    "\n",
    "    def zamena(self):\n",
    "        \"\"\"Замены символа й на и, ё на е\"\"\"\n",
    "        p = re.compile('й')\n",
    "        r = re.compile('ё')\n",
    "        res_text = []\n",
    "        for line in self.format_text:\n",
    "            new_line = []\n",
    "            for word in line:\n",
    "                word = p.sub('и', word)\n",
    "                word = r.sub('е', word)\n",
    "                new_line.append(word)\n",
    "            res_text.append(new_line)\n",
    "        self.format_text = res_text\n",
    "\n",
    "    def processing(self):\n",
    "        self.only_words()\n",
    "        self.del_word_with_n_letters(n=1)\n",
    "        #self.del_stop_words()\n",
    "        self.zamena()\n",
    "        # текст = ['строка', 'строка', ... , 'строка']\n",
    "        self.list_text = [' '.join(spisok) for spisok in self.format_text]\n",
    "\n",
    "# предобработка тренировочных данных\n",
    "X_train = Text_Preprocessor(Data_train)\n",
    "X_train.processing()\n",
    "# предобработка тестовых данных\n",
    "X_test = Text_Preprocessor(Data_test)\n",
    "X_test.processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Машинное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLearning():\n",
    "\n",
    "    def __init__(self, X_data, y_target):\n",
    "        self.X_data = X_data\n",
    "        self.y_target = y_target\n",
    "\n",
    "    def train_clf(self):\n",
    "        # объединяем векторизатор TF-IDF и классификатор SVM в единый конвейерный классификатор\n",
    "        # TfidfVectorizer => SVM => result\n",
    "        pipe = Pipeline([\n",
    "                ('vect', TfidfVectorizer(lowercase=False, analyzer='word', binary='True', token_pattern=r'\\b\\w+\\b')),\n",
    "                ('clf', svm.LinearSVC(C=5, verbose=True, max_iter=5000))\n",
    "        ])\n",
    "        # используем случайную перекрестную кросс-валидацию\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "        # набор параметров для выбора оптимального\n",
    "        param_grid = [\n",
    "            {\n",
    "                'vect__ngram_range': [(1,2)],\n",
    "                'vect__min_df': [1],\n",
    "                'vect__smooth_idf': [True],\n",
    "                'vect__sublinear_tf': [True],\n",
    "                'vect__norm' : ['l2']\n",
    "\n",
    "            }\n",
    "        ]\n",
    "        # поиск оптимальных параметров по сетке\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid, cv=cv)\n",
    "        grid.fit(self.X_data, self.y_target)\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "        \n",
    "        return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear]{'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__norm': 'l2', 'vect__smooth_idf': True, 'vect__sublinear_tf': True}\n",
      "0.935990770577\n"
     ]
    }
   ],
   "source": [
    "# обучение классификатора на тренировочных данных\n",
    "ml = MLearning(X_train.list_text, group_id_tr)\n",
    "clf = ml.train_clf()\n",
    "# предсказание классификатора на тестовых данных\n",
    "Y = clf.predict(X_test.list_text)\n",
    "# запись резульатов в файл\n",
    "pd.DataFrame({'GROUP_ID' : Y,\n",
    "              'id' : id_test})[['id', 'GROUP_ID']].to_csv(r'output\\res.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
